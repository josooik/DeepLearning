{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segmentation_basic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josooik/DeepLearning/blob/main/segmentation_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPN5JLCQLj4e"
      },
      "source": [
        "# Image Segmentation\n",
        "\n",
        "![](https://github.com/yebiny/Image-Segmentation-TF2/blob/main/imgs/seg_type.png?raw=true)\n",
        "\n",
        "\n",
        "![](https://github.com/yebiny/Image-Segmentation-TF2/blob/main/imgs/seg_model.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcwzvgrTLpI2"
      },
      "source": [
        "## 데이터셋: Crowd Instance-level Human Parsing (CIHP)\n",
        "---\n",
        "\n",
        "(https://arxiv.org/abs/1811.12596)\n",
        "\n",
        "* 38,280개의 다양한 인간 이미지\n",
        "![](https://github.com/yebiny/Image-Segmentation-TF2/blob/main/imgs/chip.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-D-z5mb3vfs"
      },
      "source": [
        "### 1. 데이터셋 다운로드\n",
        "\n",
        "* instance-level-human-parsing.zip(2.7G): https://drive.google.com/uc?id=1B9A9UCJYMwTL4oBEo4RZfbMZMaZhKJaz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgIeATGc3vfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2806de6a-74d6-4d5b-b467-6097a6b0854b"
      },
      "source": [
        "! gdown https://drive.google.com/uc?id=1B9A9UCJYMwTL4oBEo4RZfbMZMaZhKJaz"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Permission denied: https://drive.google.com/uc?id=1B9A9UCJYMwTL4oBEo4RZfbMZMaZhKJaz\n",
            "Maybe you need to change permission over 'Anyone with the link'?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Uv9NV_A9EBU",
        "outputId": "d24d7c48-90b4-4af5-b568-641fb188189d"
      },
      "source": [
        "! unzip -q instance-level-human-parsing.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open instance-level-human-parsing.zip, instance-level-human-parsing.zip.zip or instance-level-human-parsing.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orTkLuqF6U6-"
      },
      "source": [
        "### 2. 이미지 경로 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItrwStat6znm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2199bd74-361a-487a-bfef-4f68780d1116"
      },
      "source": [
        "import os \n",
        "from glob import glob\n",
        "\n",
        "# 폴더 '/content/instance-level_human_parsing/instance-level_human_parsing'\n",
        "# Training\n",
        "# Validation\n",
        "\n",
        "data_dir = '/content/instance-level_human_parsing/instance-level_human_parsing'\n",
        "\n",
        "# Training\n",
        "train_img_path = glob(os.path.join(data_dir, 'Training/Images/*.jpg'))\n",
        "train_mask_path = glob(os.path.join(data_dir, 'Training/Category_ids/*.png'))\n",
        "print(\"Training_Img :\", len(train_img_path))\n",
        "print(\"Training_Mask :\", len(train_mask_path))\n",
        "\n",
        "# Training Sort\n",
        "train_img_paths = sorted(train_img_path)\n",
        "train_mask_paths = sorted(train_mask_path)\n",
        "print(\"Training_Img_sort :\", train_img_paths[:2])\n",
        "print(\"Training_Mask_sort :\", train_mask_paths[:2])\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Validation\n",
        "valid_img_path = glob(os.path.join(data_dir, 'Validation/Images/*.jpg'))\n",
        "valid_mask_path = glob(os.path.join(data_dir, 'Validation/Category_ids/*.png'))\n",
        "print(\"Validation_Img :\", len(valid_img_path))\n",
        "print(\"Validation_Mask :\", len(valid_mask_path))\n",
        "\n",
        "# Validation Sort\n",
        "valid_img_paths = sorted(valid_img_path)\n",
        "valid_mask_paths = sorted(valid_mask_path)\n",
        "print(\"Validation_Img_sort :\", valid_img_paths[:2])\n",
        "print(\"Validation_Mask_sort :\", valid_mask_paths[:2])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training_Img : 0\n",
            "Training_Mask : 0\n",
            "Training_Img_sort : []\n",
            "Training_Mask_sort : []\n",
            "\n",
            "\n",
            "Validation_Img : 0\n",
            "Validation_Mask : 0\n",
            "Validation_Img_sort : []\n",
            "Validation_Mask_sort : []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sXQz4opIpHV"
      },
      "source": [
        "## 데이터셋 제너레이터"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0nfoPmhcznK"
      },
      "source": [
        "### 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z799rhfW6Scs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "26380083-5c0f-4e9e-dc28-312329abe9b9"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "# img size 128\n",
        "# img = img / 255\n",
        "# img = (img 127.5) - 1\n",
        "\n",
        "class DataGenerator():\n",
        "  def __init__(self, img_size, batch_size):\n",
        "    self.img_size = img_size\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def path2arr(self, path, mask=False): \n",
        "    # img 일때\n",
        "    if not mask:\n",
        "      # path - > img\n",
        "      img = load_img(path, color_mode='rgb', target_size=(self.img_size, self.img_size))\n",
        "      # img -> arr\n",
        "      arr = img_to_array(img, dtype='uint8')\n",
        "    \n",
        "    # mask 일때\n",
        "    else:\n",
        "      # path - > img\n",
        "      img = load_img(path, color_mode='grayscale', target_size=(self.img_size, self.img_size))\n",
        "      # img -> arr\n",
        "      arr = img_to_array(img, dtype='uint8')\n",
        "\n",
        "    return arr\n",
        "\n",
        "  def load_datasets(self, img_paths, mask_paths):\n",
        "    img_arrs = []\n",
        "    mask_arrs = []\n",
        "    \n",
        "    for img_path, mask_path in zip(img_paths, mask_paths):\n",
        "      img_arr = self.path2arr(img_path)\n",
        "      img_arrs.append(img_arr)\n",
        "\n",
        "      mask_arr = self.path2arr(mask_path, mask=True)\n",
        "      mask_arrs.append(mask_arr)\n",
        "    \n",
        "    return np.array(img_arrs.dtype='float32'), np.array(mask_arrs.dtype='float32')\n",
        "\n",
        "  def preprocess(self, img_arr, mask_arr):\n",
        "    img_arr = (img_arr / 127.5) -1\n",
        "\n",
        "    return img_arr, mask_arr\n",
        "\n",
        "  def tf_preprocess(self, img_arr, mask_arr):\n",
        "    img_arr = (tf.cast(img_arr, tf.float32) / 127.5) -1\n",
        "    mask_arr = tf.cast(mask_arr, tf.float32) # int -> float\n",
        "\n",
        "    return img_arr, mask_arr\n",
        "\n",
        "  def generate(self, img_paths, mask_paths):\n",
        "    # 기존 Numpy 이용한 데이터셋 처리\n",
        "    img_arrs, mask_arrs = self.load_datasets(train_img_paths, train_mask_paths)\n",
        "    img_arrs.shape, mask_arrs.shape\n",
        "\n",
        "    # tensorflow dataset 라이브러리 사용 여기서부터는 Numpy가 아니라 Tensor 이용\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((img_arrs, mask_arrs))\n",
        "    dataset = dataset.map(self.tf_preprocess, num_parallel_calls=tf.data.AUTOTUNE) # 전처리, num_parallel_calls=tf.data.AUTOTUNE -> 병렬처리\n",
        "    dataset = dataset.batch(self.batch_size, drop_remainder=True) # batch 사이즈 지정\n",
        "    print(dataset)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-bdb7ac5e77a4>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    return np.array(img_arrs.dtype='float32'), np.array(mask_arrs.dtype='float32')\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword can't be an expression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbDsSOcpEI6K"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def hist_val(img_arr, mask_arr):\n",
        "  plt.figure(figsize=(10,4))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  img_shape = img_arr.shape[0] * img_arr.shape[1] * img_arr.shape[2]\n",
        "  plt.hist(np.reshape(img_arr, img_shape)) # 1차원인풋\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  mask_shape = mask_arr.shape[0] * mask_arr.shape[1] * mask_arr.shape[2]\n",
        "  plt.hist(np.reshape(mask_arr, mask_shape)) # 1차원인풋\n",
        "  print('* # of mask categories :', set(np.reshape(mask_arr, mask_shape))) # set 중복 제거, mask의 카테고리 확인\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def display_img(img_arr, mask_arr, mask_cmap=None):\n",
        "  plt.figure(figsize=(12,4))\n",
        "\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img_arr)\n",
        "\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img_arr)\n",
        "  plt.imshow(mask_arr[:,:,0], alpha=0.6, cmap=mask_cmap) # cmap 기본은 None\n",
        "\n",
        "  plt.subplot(1,3,3)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(mask_arr[:,:,0], cmap=mask_cmap) # mask_arr (가로, 세로, 1) -> (가로, 세로)\n",
        "  \n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfGxW9OXeQAv"
      },
      "source": [
        "### 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3s7sd_ERb6d"
      },
      "source": [
        "IMG_SIZE = 256\n",
        "BATCH_SIZE = 16\n",
        "dg = DataGenerator(IMG_SIZE, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "905J01kVLelu"
      },
      "source": [
        "### 1. array 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExQGeCj0dfoA"
      },
      "source": [
        "idx = 55\n",
        "\n",
        "img_arr = dg.path2arr(train_img_paths[idx])\n",
        "mask_arr = dg.path2arr(train_mask_paths[idx], mask=True)\n",
        "\n",
        "print(img_arr.shape, mask_arr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdTtGW22HOSp"
      },
      "source": [
        "display_img(img_arr, mask_arr, mask_cmap='rainbow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWPyIEifB4Ey"
      },
      "source": [
        "hist_val(img_arr, mask_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy2lEOIuLhdW"
      },
      "source": [
        "### 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI5aDLYnerMP"
      },
      "source": [
        "img_pre, mask_pre = dg.preprocess(img_arr, mask_arr)\n",
        "print(img_pre.shape, mask_pre.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSKM87sye4v-"
      },
      "source": [
        "display_img(img_pre, mask_pre, mask_cmap='rainbow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG67E4sbfAE_"
      },
      "source": [
        "hist_val(img_pre, mask_pre)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIHQL4bSzQpG"
      },
      "source": [
        "### 3. 데이터셋 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us-L3SQg7aDz"
      },
      "source": [
        "train_ds = dg.generate(train_img_paths[:5000], train_mask_paths[:5000])\n",
        "valid_ds = dg.generate(valid_img_paths, valid_mask_paths)\n",
        "\n",
        "#train_img, train_mask = dg.load_datasets(train_img_paths[:10000], train_mask_paths[:10000])\n",
        "#valid_img, valid_mask = dg.load_datasets(valid_img_paths, valid_mask_paths)\n",
        "\n",
        "print(\"* Train ds : \", train_ds.shape)\n",
        "print(\"* Valid ds :\", valid_ds.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prb3QskIL6E8"
      },
      "source": [
        "## 모델: UNet\n",
        "---\n",
        "\n",
        "![]( https://github.com/yebiny/Image-Segmentation-TF2/blob/main/imgs/unet2.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkytxUE_M6tp"
      },
      "source": [
        "### 1. 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WIhRhyRUFq4"
      },
      "source": [
        "from tensorflow.keras import layers, models, Input, utils\n",
        "\n",
        "def get_unet(img_size, num_classes):\n",
        "  inputs = Input(shape=( img_size, img_size, 3) )\n",
        "\n",
        "  x = layers.Conv2D(32, 3, strides=2, padding='same')(inputs)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation('relu')(x)\n",
        "  x_pre = x\n",
        "\n",
        "  for filters in [64, 128, 256]:\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.SeparableConv2D(filters, 3, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.SeparableConv2D(filters, 3, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "    residual = layers.Conv2D(filters, 1, strides=2, padding='same')(x_pre)\n",
        "    x = layers.add([x, residual])\n",
        "    x_pre = x\n",
        "\n",
        "  for filters in [256, 128, 64, 32]:\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Conv2DTranspose(filters, 3, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Conv2DTranspose(filters, 3, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "    residual = layers.UpSampling2D(2)(x_pre)\n",
        "    residual = layers.Conv2D(filters, 1, padding='same')(residual)\n",
        "    x = layers.add([x, residual])\n",
        "    x_pre = x\n",
        "\n",
        "  outputs = layers.Conv2D(num_classes, 3, activation='softmax', padding='same')(x)\n",
        "\n",
        "  model = models.Model(inputs, outputs)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D4Jhcp7M84L"
      },
      "source": [
        "### 2. 모델 컴파일"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA38g-grUNf2"
      },
      "source": [
        "NUM_CLASSES = 20\n",
        "model = get_unet(IMG_SIZE, NUM_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sumB2qzUkhUd"
      },
      "source": [
        "tf.keras.utils.plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Igi_n1JKxG3"
      },
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learing_rate=0.001)\n",
        "\n",
        "model.compile(loss=loss,\n",
        "              optimizer = optimizer,\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x34P3CRM-cL"
      },
      "source": [
        "### 3. 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1gp_qzBlXbV"
      },
      "source": [
        "ckp = tf.keras.callbacks.ModelCheckpoint(\"unet_chip.h5\", save_best_only=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                                 factor=0.2, # 0.2씩 lr 줄이기\n",
        "                                                 patience=5, # 5번동안 loss 안줄면)\n",
        "history = model.fit(train_ds,\n",
        "                    validation_data = valid_ds,\n",
        "                    epochs=20,\n",
        "                    callbacks=[ckp]) # 학습성킁 향상 시키는 모듈"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIgiukuJTRd0"
      },
      "source": [
        "### 4. 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awiPm4kth_Pv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1HL3hfQiNLL"
      },
      "source": [
        "### 5. 추론"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp4QJEzTdXuq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}